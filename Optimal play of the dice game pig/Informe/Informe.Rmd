---
title: "Optimal play of the dice game pig"
author: "Luis Ramos Grados, Carlos Alberto Espinoza Mansilla"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introducci?n
Nuestro objetivo con este proyecto es encontrar una forma ?ptima de jugar(y ganar) el "play of the dice game pig" para esto es necesario conocer las reglas del juego:

El juego inicia

* El primer jugador tira el dado
* Si saca 1 el puntaje acumulado durante su turno se vuelve cero y su turno termina 
* Si saca de 2 a 6 el resultado se suma al puntaje de ese turno
* Ahora el jugador decide si vuelve a tirar el dado o terminar su turno
* Si el jugador termina su turno el puntaje acumulado durante su turno se suma al puntaje del jugador
* Si el jugador decide volver a lanzar se repetira el proceso hasta que decida terminar su turno o saque 1
*El juego termina cuando un jugador logra acumular 100 o mas puntos.

A pesar de que este juego tiene muchas mas variaciones y puede jugarse de 2 a 10 jugadores aqui tomaremos la forma estandar del juego que es la ya descrita y jugada por 2 personas con un dado de 6 lados justo. Trataremos de maximizar nuestrar probabilidades de ganar usando nuestro conocimientos sobre probabilidades y luego computaremos los algoritmos que sean necesarios para resolver el problema con las herramientas que nos da R.


## Estado del arte

1. Practical Play of the Dice Game Pig por Todd W. Neller, Clifton G.M. Presser: Siendo conciente de lo complicado del resultado "?ptimo" para que un humano lo aplique en un entorno casual, busca una forma que sin ser la mas ?ptima resulta mas conveniente para jugar de forma optimo de modo casual.

2. Pig (Pig-out): An?lisis de una de las variaciones del juego original esta vez con 2 dados.

## Dise?o del experimento

Primero vamos a definir de que cosas depende la probabilidad de ganar de nuestro jugador:
Para un turno cualquiera de nuestro jugador la posibilidad de ganar va a depender de cuantos puntos tengemos (i), de cuantos puntos tiene nuestro oponente (j), de cuantos puntos tenemos acumulados en nuestro turno (k) y de decidir si volver a lanzar el dado o terminar el turno dependiendo de cual me da mas posibilidades de ganar, entonces definimos nuestras posibilidades de ganar como:

* $P_{i,j,k}=max(P_{i,j,k,lanzar},P_{i,j,k,terminar})$ 

Donde $P_{i,j,k}$ es la posibilidad de ganar total, $P_{i,j,k,lanzar}$ es la posibilidad de ganar lanzando de nuevo el dado y $P_{i,j,k,terminar}$ es la posibilidad de ganar terminando ese turno.

Ahora si consideramos la situaci?n en la que $i+k\geq 100$ entonces $P_{i,j,k}=1$ porque le basta al jugador con terminar all? su turno para ganar, tambi?n podemos establecer limites para i,j,k:

* $0\leq i < 100$ , $0\leq j < 100$ y $0\leq k < 100-i$

ahora la probabilidad de ganar terminando nuestro turno va a ser igual a la probabilidad de que nuestro oponente no gane:

* $P_{i,j,k,terminar}=1-P_{j,i+k,0}$ 

Mientras que la probabilida de ganar volviendo a lanzar el dado es la suma de las probabilidades de ganar obteniendo (1,2,3,4,5,6) multiplicado por la posibilidad de que se de dicho resultado:
 
* $P_{i,j,k,lanzar}=[\frac{1}{6}P_{i,j,k+1}+\frac{1}{6}P_{i,j,k+2}+\frac{1}{6}P_{i,j,k+3}+\frac{1}{6}P_{i,j,k+4}+\frac{1}{6}P_{i,j,k+5}+\frac{1}{6}P_{i,j,k+6}]$

pero $P_{i,j,k+1}$ significa que todo el k acumulado se reduce a 0 y termina tu turno por lo $P_{i,j,k+1} = 1-P_{j,i,0}$ ahora:

* $P_{i,j,k,lanzar}=\frac{1}{6}[(1-P_{j,i,0})+P_{i,j,k+2}+P_{i,j,k+3}+P_{i,j,k+4}+P_{i,j,k+5}+P_{i,j,k+6}]$

Bueno, ya hemos construido un modelo matem?tico para calcular la maxima probabilidad de ganar para cada jugada posible solo queda resolver.

## Experimentos y resultados

Para resolver $P_{i,j,k}=max(P_{i,j,k,lanzar},P_{i,j,k,terminar})$ Notamos que nuestro modelo tiene resultados que dependen en parte de la probabilidad (resultado del dado), en parte de la decision que tome el jugado (lanzar el dado o terminar su turno) vemos que tal cual descrito, nuestro modelo se adecua a un MDP (Markov decision process), esto quiere decir que podemos intentar resolverlo mediante uno de los metodos conocidos de soluci?n para MDP en este caso usaremos "VALUE ITERATION".

#### Value Iteration

Value iteration es un algoritmo que de forma ciclica estima valores para cada estado de un problema hasta llegar a un valor muy aproximado al ?timo. Ahora vamos a considerar la divisi?n del problema en estados(s),acciones(a) y recompensas(r) donde para nuestro problema los estados deben ser entendidos como cada situacion posible durante el juego para los i,j,k ,por ejemplo un estado seria la situaci?n donde tengo i=96,j=99,k=2 , tengo 98 puntos , mi oponente 99 y tengo acumulado en ese turno 2 puntos, las acciones son lanzar el dado o terminar el turno y recompensa se define como un valor n?merico que sirve para determinar que cambio de estado es mas optimo.
Si se tiene 2 estados $s,s^{'}\in S$ y tomando la acci?n $a\in A$ se sabe que hay una $P^{a}_{ss'}$  de que haya un cambio de estado de s a s' a este resultado se le asocia la recompensa $R^{a}_{ss'}$. Tambi?n es necesario definir V(s) como el valor de estar en el estado "s" basado en las recompensas necesarias para llegar a ese estado y los valores de los estados previos, decimos entonces que el valor estimado de una acci?n a en el estado s es dado por:

* $\sum_{s'}P^a_{ss'}[R^a_{ss'}+ \gamma V(s')]$

y entonces la opcion optima es:

* $max_{a\in A}(\sum_{s'}P^a_{ss'}[R^a_{ss'}+ \gamma V(s')])$

Donde $0\leq\gamma<1$ es llamado factor de descuento en indica que tanto nos interesa la recompensa futura de escoger la acci?n a.

Aplicando esto a nuestro problema es:

* $max(P_{i,j,k,lanzar},P_{i,j,k,terminar})$

ya que vimos que solo hay 2 acciones y consideramos que la recompensa solo existe y es igual a 1 si se pasa de "no ganar" a "ganar",ademas de dar $\gamma = 1$

Ahora usamos el algoritmo "value update" o "Bellman update/back-up"

*** 
#### Algoritmo "Bellman update/back-up"  
Para cada $s\in S$, inicializa V(s) aleatoriamente  
Repetir:  
----------$\triangle <-0$  
----------Para cada $s \in S$  
--------------------$v<-V(s)$  
--------------------$V(s)<-max_{a\in A}(\sum_{s'}P^a_{ss'}[R^a_{ss'}+ \gamma V(s')])$  
--------------------$\triangle<-max(\triangle,|v-V(s)|)$  
hasta que $\triangle<e$  
donde $0< e <1$ indica la aproximaci?n a los valores ?ptimos

***

Aplicar el algoritmo directamente al "play of the dice game pig" resulta muy complicado por lo que aqu? se har? primero un ejemplo con un juego mucho mas simple.

Definamos pues las reglas basicas de modo similar al "play of the dice game pig"

* jugador lanza moneda, si sale cruz termina su turno
* si sale cara su puntaje de turno sube 1 punto y puede elegir si seguir lanzando o terminar su turno
* gana qui?n llegue a 2 puntos primero

El ejemplo esta dado para que sea una versi?n mas simple y nos permita guiarnos a la soluci?n buscada. Definiendo i como puntaje del jugador, j como puntaje del oponente y k como puntaje de turno, en el caso que i+k=2 $P_{i,j,k}=1$ en el resto de casos donde 0< i,j < 2 y k<2-i , la probabilidad de ganar es:

* $max(P_{i,j,k,lanzar},P_{i,j,k,terminar})$

Donde:
* $P_{i,j,k,terminar} = 1 -P_{j,i,0} $
* $P_{i,j,k,lanzar} = \frac{1}{2}[1-P_{j,i,0}+P_{i,j,k+1}]$

luego todos los estados posibles son:

* $P_{0,0,0} = max(\frac{1}{2}[1-P_{0,0,0}+P_{0,0,k+1}],1 -P_{0,0,0})$

* $P_{0,0,1} = max(\frac{1}{2}[1-P_{0,0,0}+1],1 -P_{0,1,0})$

* $P_{0,1,0} = max(\frac{1}{2}[1-P_{1,0,0}+P_{0,0,k+1}],1 -P_{1,0,0})$

* $P_{0,1,1} = max(\frac{1}{2}[1-P_{1,0,0}+1],1 -P_{1,1,0})$

* $P_{1,0,0} = max(\frac{1}{2}[1-P_{0,1,0}+1],1 -P_{0,1,0})$

* $P_{1,0,1} = 1$

* $P_{1,1,0} = max(\frac{1}{2}[1-P_{1,1,0}+1],1 -P_{1,1,0})$

* $P_{1,1,1} = 1$

Implementando el algoritmo: Codigo fuente en Codigo_Fuente/'value iteration'.R 
```{r echo=FALSE}
#establecemos valores posibles para nuestros indices i,j,k
#a pesar de que en el problema se trabaja con valores 0 y 1 R no permite esto asi que se trabajara con valores 1,2:
I=c(1,2) #Para i
J=c(1,2) #Para j
K=c(1,2) #Para k

S <-array(NA, c(2,2,2)) #seteamos el array de probabilidad.

#El estado "S" es tu posicion en i,j,k en la matriz de 3 dimensiones.
#Donde i representa el puntaje acumalado en el juego, j el puntaje acumulado del oponente durante el juego
#y k es el puntaje acumulado durante su turno, y el contenido S(i,j,k)
#es la probabilidad de ganar a partir de ese estado
#Existen 2 acciones:  "Lanzar" y "terminar turno"


#Rellenamos el array S con valores arbitrarios:

for (i in I) {
  for (j in J) {
    for (k in K) {
      #Si se tiene el estado en el que ya se ha ganado,es decir S(i,j,k)=1, se le da 1
      if(i+k>=4)
        S[i,j,k] <- 1
      #Caso contrario, se le asigna una numero aleatorio entre [0,1]
      else
        S[i,j,k] <- sample((1:999),1)/1000
    }
  }
}
#Mostramos el arreglo generado
S

#La indexacion en R empieza desde 1, por lo cual interpretaremos el ?ndice S[1,2,1] como S[0,1,0]

e=0.001  #seteamos el m?nimo de aproximaci?n

P=as.vector(S) # inicializamos el vector que contendra los resultados de todas las iteraci?nes
P
iteracion=0

m=0

x=1  #seteamos la condicion inicial 

while (x >= e) { #iniciamos el bucle
  x = 0
  
  Pr=NULL #seteamos el vector que guardara los cambios por cada iteracion
  
  for (i in I) {
    for (j in J) {
      for (k in K) {
        
        v = S[i,j,k]
        
        if(i+k == 4 ) # Si entra un S(i,j,k) correspondiente a un estado ganador
          S[i,j,k] = 1
        else 
          if(k+1 == 3) # si entra un S(i,j,k) que pueda salirse de los limites de nuestra matriz(2,2,2)
            S[i,j,k] = max( 0.5*(1-S[j,i,1]+1),1-S[j,i,1])
          else         # todos los demas imputs
          S[i,j,k] = max( 0.5*(1-S[j,i,1]+S[i,j,k+1]),1-S[j,i,1]) 
        
        x = max(x,abs(v-S[i,j,k]))
        Pr=c(Pr,S[i,j,k])
        
      }
    }
  }
  
  m=m+1
  iteracion=c(iteracion,m)
  P=rbind(P,Pr)
}

plot(iteracion,P[,1],type="o",col="blue",xlab="nro de iteraciones",ylab = "Probabilidad de ganar",main = "value iteration para piglet")
lines(iteracion,P[,2],type="o",col="green")
lines(iteracion,P[,3],type="o",col="red")
lines(iteracion,P[,4],type="o",col="yellow")
lines(iteracion,P[,5],type="o",col="black")
lines(iteracion,P[,7],type="o",col="orange")
legend("bottomright",col=c("blue","green","red","yellow","black","orange"),legend=c("P(0,0,0)","P(0,0,1)","P(0,1,0)","P(0,1,1)","P(1,0,0)","P(1,1,0)"),lwd=3,bty="n")


for(i in I)
  for(j in J)
    for(k in K)
      cat(sprintf("P(%d,%d,%d)=%f\n", i-1,j-1,k-1,S[i,j,k]))

```

### Aplicando Value iteration al "play of the dice game pig"

Ahora que ya tenemos el algoritmo implementando y una mejor comprension de como funciona vamos a aplicarlo a nuestro objetivo principal el "play of the dice game pig"

Podemos aplicar el metodo Value Iteration para solucionar las ecuaciones que tenemos para el Dice game pig, un problema seria que la cantidad de ecuaciones que tendriamos son 505000. Esta cantidad de ecuaciones hace que sea muy lento el proceso para llegar a la convergencia de los valores.
Analizando los estados posible, nos damos cuenta que para pasar de un estado a otro, el estado inicial no tiene un puntaje acumaulado mayor al del estado transitado, pues en el juego no se pierde puntos acumulados. De esto podemos decir que la probabilidad de ganar en un estado con una suma puntajes acumulados dado, es decir i+j, no depende de las probabilidades de otros estados con menor suma de puntajes acumulados.
Esto nos permite calcular primero con el metodo solo la probabilidad de estados con mayor suma de puntajes acumulados.
Asi podemos calcular la probabilidad de un estado con una suma de puntajes dado y luego calcular las probabilidades de ganar de un estado con menor suma puntajes acumulados al anterior estado con la probabilidad del estado ya calculado.

Para nuestro juego calculariamos primero $P_{99,99,0}$,cuya suma es 198, luego calculariamos $P_{99,98,0}$ y $P_{98,99,0}$ cuyas sumas son 197 y asi para estados cuyas sumas sean 196, 195, etc., hasta que finalmente calculemos $P_{0,0,k}$ donde $0 \le k \le 99 $

Tambien podemos notar que, la probabilidad de un estado con suma de puntajes acumulados dado depende de
* estados con mayor suma de puntajes acumulados o
* estados con puntajes intercabiados (lanzar 1 o terminar turno)

Entonces calcularemos con value iteration lo siguiente: Para un estado dado con puntajes i,j calcularemos $P_{i,j,k}$ donde $0 \le k \le 100-i$ y $P_{j,i,k}$ donde $0 \le k \le 100-j$

## Discusi?n

## Conclusiones

## Bibliograf?a o Referencias

[1] http://cs.gettysburg.edu/~tneller/papers/umap10.pdf Practical Play of the Dice Game Pig por Todd W. Neller

[2] http://www.durangobill.com/Pig.html Pig (Pig-out)

[3] https://people.eecs.berkeley.edu/~pabbeel/cs287-fa12/slides/mdps-exact-methods.pdf

[4] https://people.eecs.berkeley.edu/~pabbeel/cs287-fa12/slides/mdps-exact-methods.pdf

[5]https://en.wikipedia.org/wiki/Markov_decision_process 